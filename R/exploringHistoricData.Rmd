---
title: "Exploring historic data"
author: "Vanuatu National Statistics Office"
date: "`r format(Sys.Date(), '%d %b %Y')`"
output:
  html_document:
    toc: yes
    toc_depth: '2'
    toc_float: yes
params: 
  password: "ThisIsNotMyPassword"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r preparation, include=FALSE}
# Load the required libraries
library(RMySQL) # Interactiing with MySQL from R
library(knitr) # Nice tables
library(kableExtra) # Extra nice tables
library(basicPlotteR) # Progress bar
library(plotly) # Interactive graphs
library(openxlsx) # Read and write excel formatted files
library(RColorBrewer) # Creating colour pallettes
library(basicPlotteR) # Set transparency of colours

# Set the number of digits to use before using exp notation
options("scipen"=50)

# Load the general R functions
source("functions.R")

# Get the repository directory - for building relative paths
repository <- dirname(getwd())
```

# Introduction

Here we document the exploration of the historic trade statistics data for Vanuatu. Our aim is to understand how the value of different imported and exported commodities has changed through time. Due to the large nature of the historic data, these data are stored on a [MySQL](https://www.mysql.com/) server. We will use the [RMySQL](https://cran.r-project.org/web/packages/RMySQL/RMySQL.pdf) R package to interact with the local MySQL server from R.

To set up the local MySQL server, a dump of the VNSO server was shared and imported via the [MySQL Workbench](https://www.mysql.com/products/workbench/) on my computer. I had previously install MySQL by following these instructions:
- Installing on linux systems ([link](https://dev.mysql.com/doc/refman/8.0/en/linux-installation.html)), with extra help [here](https://itsfoss.com/install-mysql-ubuntu/).
- Installing on mac: ([link](https://dev.mysql.com/doc/mysql-osx-excerpt/5.7/en/osx-installation.html)), also, I think it comes ready installed! See [here](https://www.thoughtco.com/installing-mysql-on-mac-2693866))
- Installing on Windows ([link](https://dev.mysql.com/downloads/installer/)), with extra help [here](https://www.wikihow.com/Install-the-MySQL-Database-Server-on-Your-Windows-PC)

# Connecting to the database

The MySQL server is running locally and we can connect to it from R with the following code:

```{r connection, eval=FALSE}
# Open a connection to a MySQL database
connection <- dbConnect(MySQL(), 
                            user='JosephCrispell', 
                            password=readline(prompt="Enter password: "), # Doing this as password never stored in easily accessible format
                            dbname='vnso',
                            host='localhost')
```


```{r echo=FALSE}
# Open a connection to a MySQL database
connection <- dbConnect(MySQL(), 
                        user='JosephCrispell', 
                        password=params$password, # Doing this as password never stored in easily accessible format
                        dbname='vnso',
                        host='localhost')
```

```{r get size of tables, echo=FALSE}
nImportRecords <- dbGetQuery(conn=connection, statement="SELECT TABLE_ROWS FROM information_schema.TABLES WHERE table_name = 'historical_import_99_19'") # Slower more accurate query: SELECT COUNT(*) FROM historical_import_99_19
nExportRecords <- dbGetQuery(conn=connection, statement="SELECT TABLE_ROWS FROM information_schema.TABLES WHERE table_name = 'historical_export_99_19'") # Slower more accurate query: SELECT COUNT(*) FROM historical_export_99_19
```


On our local MySQL database there are two tables:

- `historical_export_99_19` - the historic data for all exports (~`r nExportRecords` rows)
- `historical_import_99_19` - the historic data for all imports (~`r nImportRecords` rows)

The imports data looks like this:

```{r quick look at historic imports data, echo=FALSE}
# Extract a small subset of historic data
historic_imports <- dbGetQuery(conn=connection, statement="SELECT * FROM historical_import_99_19 LIMIT 25")

# View the table
prettyTable(historic_imports)
```

<br>
And the exports data like this:
<br>

```{r quick look at historic exportsdata, echo=FALSE}
# Extract a small subset of historic data
historic_exports <- dbGetQuery(conn=connection, statement="SELECT * FROM historical_export_99_19 LIMIT 25")

# View the table
prettyTable(historic_exports)
```

# Summarise the `Value` distribution for each HS code

Each commodity in the historic data is identified by an 8 digit **HS** (Harmonized System) code. We are aiming to define the expected value distribution for each unique **HS** code through time for imports and exports. 

Firstly we need to calculate the unit value by dividing `Value` column by the `Weight` column for both the imports and exports:
```{r create column to store value divded by weight}
# Add column to unit value - Value column by Weight into IMPORTS table
colNames_imports <- dbGetQuery(connection, statement="SELECT COLUMN_NAME FROM INFORMATION_SCHEMA.COLUMNS WHERE TABLE_SCHEMA='vnso' AND TABLE_NAME='historical_import_99_19'")
if("UnitValue" %in% colNames_imports[, 1] == FALSE){
  dbSendQuery(connection, statement="ALTER TABLE historical_import_99_19 ADD UnitValue DOUBLE AS (Value / Weight)")
}

# Add column for unit value - Value column by Weight into EXPORTS table
colNames_exports <- dbGetQuery(connection, statement="SELECT COLUMN_NAME FROM INFORMATION_SCHEMA.COLUMNS WHERE TABLE_SCHEMA='vnso' AND TABLE_NAME='historical_export_99_19'")
if("UnitValue" %in% colNames_exports[, 1] == FALSE){
  dbSendQuery(connection, statement="ALTER TABLE historical_export_99_19 ADD UnitValue DOUBLE AS (Value / Weight)")
}
```

Using `MySQL` commands we can generate simple summary statistics of the `UnitValue` column by HS code and year. For the exports, for example:

```{r summarise unit value column}
# Summarise the unit value column for each commodity for each year
commoditySummaryExports <- dbGetQuery(conn=connection, statement="SELECT HS, YEAR, Description, CTY_Dest AS CTY, AVG(UnitValue) AS average, MAX(UnitValue) AS maximum, MIN(UnitValue) AS minimum, COUNT(UnitValue) AS count, SUM(UnitValue) AS sum, STDDEV_SAMP(UnitValue) AS sd, VAR_SAMP(UnitValue) as variance FROM historical_export_99_19 GROUP BY HS, YEAR")

# View the table
prettyTable(commoditySummaryExports[1:25, ])
```

Ideally though, we'd like to generate some additional summary statistics (like percentiles and median), particularly since the value of commodities is often highly skewed (with some rare items costing a huge amount). We can pull the import and export data into R to work with it directly:

```{r load all imports and exports}
# Get the historic data
exports <- dbGetQuery(conn=connection, statement="SELECT * FROM historical_export_99_19")
imports <- dbGetQuery(conn=connection, statement="SELECT * FROM historical_import_99_19")

# Remove data outside of 1999-2019
imports <- imports[imports$Year >= 1999 & imports$Year <= 2019, ]
exports <- exports[exports$Year >= 1999 & exports$Year <= 2019, ]
```

```{r store local copy, echo=FALSE, eval=FALSE}
# Save the data as a standard csv
write.table(exports, file=file.path("..", "data", "secure", "tradeStats_historic_EXPORTS_14-09-20.csv"),
            sep=",", row.names=FALSE, quote=TRUE)
write.table(imports, file=file.path("..", "data", "secure", "tradeStats_historic_IMPORTS_14-09-20.csv"),
            sep=",", row.names=FALSE, quote=TRUE)
```

With these data loaded we'll use the following function to calculate a range of summary statistics:
```{r summary statistics function, eval=FALSE}
calculateSummaryStatistics <- function(values){
  
  # Create an output vector to store the summary statistics
  output <- c("Mean"=NA, "SD"=NA, "Median"=NA, "Lower-2.5"=NA, "Upper-97.5"=NA,
              "Lower-1"=NA, "Upper-99"=NA,
              "Min"=NA, "Max"=NA, "Count"=NA, "CountMissing"=NA)
  
  # Count number of values and any missing data
  output["Count"] <- length(values)
  output["CountMissing"] <- sum(is.na(values) | is.infinite(values) | is.nan(values))
  
  # Check if only missing available - if so stop and return empty summary statistics
  if(output["CountMissing"] == output["Count"]){
    return(output)
  }
  
  # Calculate mean
  output["Mean"] <- mean(values, na.rm=TRUE)
  
  # Calculate standard deviation
  output["SD"] <- sd(values, na.rm=TRUE)
  
  # Calculate median
  output["Median"] <- median(values, na.rm=TRUE)
  
  # Calculate upper and lower 95% percentile bounds
  quantiles <- quantile(values, probs=c(0.99, 0.975, 0.025, 0.01), na.rm=TRUE)
  output["Upper-99"] <- quantiles[1]
  output["Upper-97.5"] <- quantiles[2]
  output["Lower-2.5"] <- quantiles[3]
  output["Lower-1"] <- quantiles[4]
  
  # Calculate the range of the data
  minMax <- range(values, na.rm=TRUE)
  output["Min"] <- minMax[1]
  output["Max"] <- minMax[2]
  
  return(output)
}
```

With the function above we can use *vector operations* in R to quickly calculate summary statistics for both the import and export data. We are doing the calculations by HS code alone, by HS code and Year, and for both we calculate summary of `Value` and `UnitValue` columns:

```{r calculate summary statistics for imports and exports, eval=FALSE}
# Generate summary statistics for EXPORTS by HS code and year
exportsByHSCodeAndYear <- do.call(data.frame,
                                  aggregate(exports[, c("Value", "UnitValue")], by=list(exports$HS, exports$Year),
                                            FUN=calculateSummaryStatistics))
colnames(exportsByHSCodeAndYear)[1:2] <- c("HS", "Year")

# Generate summary statistics for EXPORTS by HS code
exportsByHSCode <- do.call(data.frame,
                                  aggregate(exports[, c("Value", "UnitValue")], by=list(exports$HS),
                                            FUN=calculateSummaryStatistics))
colnames(exportsByHSCode)[1] <- c("HS")

# Generate summary statistics for IMPORTS by HS code and year
importsByHSCodeAndYear <- do.call(data.frame,
                                  aggregate(imports[, c("Value", "UnitValue")], by=list(imports$HS, imports$Year),
                                            FUN=calculateSummaryStatistics))
colnames(importsByHSCodeAndYear)[1:2] <- c("HS", "Year")

# Generate summary statistics for IMPORTS by HS code
importsByHSCode <- do.call(data.frame,
                                  aggregate(imports[, c("Value", "UnitValue")], by=list(imports$HS),
                                            FUN=calculateSummaryStatistics))
colnames(importsByHSCode)[1] <- c("HS")
```

```{r store local copy of summary statistics, echo=FALSE, eval=FALSE}
# Write the tables to file
write.table(exportsByHSCodeAndYear, file=file.path("..", "data", "secure", "exports_HS-Year_summaryStats_02-10-20.csv"),
            sep=",", row.names=FALSE, quote=TRUE)
write.table(exportsByHSCode, file=file.path("..", "data", "secure", "exports_HS_summaryStats_02-10-20.csv"),
            sep=",", row.names=FALSE, quote=TRUE)
write.table(importsByHSCodeAndYear, file=file.path("..", "data", "secure", "imports_HS-Year_summaryStats_02-10-20.csv"),
            sep=",", row.names=FALSE, quote=TRUE)
write.table(importsByHSCode, file=file.path("..", "data", "secure", "imports_HS_summaryStats_02-10-20.csv"),
            sep=",", row.names=FALSE, quote=TRUE)
```

```{r read in locally stored summary statistics, echo=FALSE}
exportsByHSCodeAndYear <- read.csv(file.path("..", "data", "secure", "exports_HS-Year_summaryStats_02-10-20.csv"))
exportsByHSCode <- read.csv(file.path("..", "data", "secure", "exports_HS_summaryStats_02-10-20.csv"))
importsByHSCodeAndYear <- read.csv(file.path("..", "data", "secure", "imports_HS-Year_summaryStats_02-10-20.csv"))
importsByHSCode <- read.csv(file.path("..", "data", "secure", "imports_HS_summaryStats_02-10-20.csv"))
```

Now we have a range of summary statistics for the imports:
```{r view import summary, echo=FALSE}
prettyTable(importsByHSCode[1:25, ])
```

and exports:
```{r view export summary, echo=FALSE}
prettyTable(importsByHSCode[1:25, ])
```

# Considering summary statistics

When generating summary statistics, we must carefully consider their appropriateness. For example, let's look at the value distribution for imported Wine (HS code: `22042100`):

```{r visualise value distribution for wine, echo=FALSE}
# Prepare for subplot
par(fig=c(0,1,0,1), bg="white")

# Get distribution of values for wine
wineValues <- imports[imports$HS == "22042100", "UnitValue"]

# Generate summary statistics
summaryStatistics <- calculateSummaryStatistics(wineValues)

# Plot a subset of the distribution
histogram <- hist(wineValues[wineValues < 5000], las=1, main="Unit value of imported wine", xlab="Unit value", breaks=50)

# Overlay mean and median
maxCount <- max(histogram$counts)
lines(x=rep(summaryStatistics["Mean"], 2),
      y=c(0, 0.2*maxCount),
      col="red")
text(x=summaryStatistics["Mean"], y=0.2*maxCount, labels="Mean", col="red", pos=4, cex=0.75, offset=0.2)
lines(x=rep(summaryStatistics["Median"], 2),
      y=c(0, maxCount),
      col="blue")
text(x=summaryStatistics["Median"], y=maxCount, labels="Median", col="blue", pos=4, cex=0.75, offset=0.2)

# Overlay percentiles
text(x=summaryStatistics["Upper-97.5"], y=1450, labels="95% boundaries", col="blue", pos=2, cex=0.75, offset=0.2)
rect(xleft=summaryStatistics["Lower-2.5"], xright=summaryStatistics["Upper-97.5"],
     ybottom=1000, ytop=1250, col=rgb(0,0,1, 0.5), xpd=TRUE, border=NA)
text(x=3750, y=950, labels="Mean +/- Standard Deviation", col="red", cex=0.75, offset=0.2)
rect(xleft=max(0, (summaryStatistics["Mean"] - summaryStatistics["SD"])), 
     xright=min(max(histogram$breaks), (summaryStatistics["Mean"] + summaryStatistics["SD"])),
     ybottom=500, ytop=750, col=rgb(1,0,0, 0.55), xpd=TRUE, border=NA)

# Set the inset figure position
par(fig=c(0.7, 0.95, 0.4, 0.9), new=TRUE) # Axis limits: x1, x2, y1, y3

# Set up sub plot plotting
par(mar=c(4, 0, 0, 0), mgp=c(0,0.5,0))

# Plot the distribution
hist(wineValues, las=1, yaxt="n", xaxt="n", ylab="", xlab="", main="")
axis(side=1, at=c(5000, 1000000, 5000000), labels=c("5000", "1M", "5M"), cex.axis=0.6)
```

The distribution for the unit value of wine is highly skewed, with values ranging up to 5 million! The above plot illustrates how summary statistics such as `mean` and `standard deviation` are strongly influenced by the rarer values of a skewed distribution. In contrast, the `median` and `percentiles` provide a more representative summary of our skewed distribution.

When we are defining our expected boundaries for the value of a commodity, we should use percentiles. Here we have calculated the 95% bounds of the distribution, we expect the value of a commodity to fall within these bounds 95% of the time.

# Track trends in the principle imports and exports {.tabset}

The principle imports and exports are defined within the [`data/open/OPN_FINAL_ASY_PrincipleCommoditiesClassifications_31-01-20.xlsx`](https://github.com/Vanuatu-National-Statistics-Office/vnso-RAP-tradeStats-materials/blob/master/data/open/OPN_FINAL_ASY_PrincipleCommoditiesClassifications_31-01-20.xlsx) excel file. These define groups of **HS** codes that together represent the top imports and exports for Vanuatu. 

The first thing to note is that each principle commodity, defined by a description, can represent multiple HS codes. We'll read in the classifications and merge them with the historic trades data:
```{r read in the principle commodities table, echo=FALSE}
# Read in the principle commodities table
principleCommodityClassifications <- read.xlsx(file.path(repository, "data", "open", "OPN_FINAL_ASY_PrincipleCommoditiesClassifications_31-01-20.xlsx"))

# Merge the descriptions with the imports and exports
imports <- merge(imports, principleCommodityClassifications, by.x="HS", by.y="HS.Code")
exports <- merge(exports, principleCommodityClassifications, by.x="HS", by.y="HS.Code")

# Subset the information for the principle commodities only
principleImports <- imports[is.na(imports$PRINCIPAL.IMPORTS) == FALSE, ]
principleExports <- exports[exports$PRINCIPAL.EXPORTS != "OTHER PRODUCTS", ]
```

```{r generate summary statistics for principle imports and exports}
# Summarise the value of the principle imports
principleImportsSummaryStats <- do.call(data.frame,
                                        aggregate(principleImports[, c("Value", "UnitValue")], by=list(principleImports$PRINCIPAL.IMPORTS, principleImports$Year),
                                                  FUN=calculateSummaryStatistics))
colnames(principleImportsSummaryStats)[1:2] <- c("Description", "Year")

# Summarise the value of the principle exports
principleExportsSummaryStats <- do.call(data.frame,
                                        aggregate(principleExports[, c("Value", "UnitValue")], by=list(principleExports$PRINCIPAL.EXPORTS, principleExports$Year),
                                                  FUN=calculateSummaryStatistics))
colnames(principleExportsSummaryStats)[1:2] <- c("Description", "Year")
```

We can now explore how the value of each principle commodity has changed through time:

## Trends in the value of exports

```{r visualise principle commodity exports trends, echo=FALSE}
# Products to highlight
exportsToHighlight <- c("BEEF", "COPRA", "KAVA")

# Start a plotly figure
fig <- plot_ly()

# Get the descriptions of principle commodities
descriptionsExports <- unique(principleExportsSummaryStats$Description)
descriptionsExportsShort <- substr(descriptionsExports, 1, 25)

# Define a colour for each principle commodity
palette <- colorRampPalette(brewer.pal(8, "Dark2"))
coloursExports <- palette(length(descriptionsExports)) 

# Add a trace for each principle commodity
for(index in seq_along(descriptionsExports)){
  
  # Get the data for current commodity
  summaryStats <- principleExportsSummaryStats[principleExportsSummaryStats$Description == descriptionsExports[index], ]
  
  # Add a trace for current commodity
  labels <- paste("<b>Year</b>:", summaryStats$Year,
                  "<br>Median: ", round(summaryStats$UnitValue.Median, digits=0),
                  "<br>Upper 97.5%: ", round(summaryStats$UnitValue.Upper.97.5, digits=0),
                  "<br>Lower 2.5%: ", round(summaryStats$UnitValue.Lower.2.5, digits=0))
  fig <- add_lines(fig, 
                     x=summaryStats$Year,
                     y=summaryStats$UnitValue.Median,
                     name=descriptionsExportsShort[index], 
                     hovertemplate=labels,
                     line=list(color=coloursExports[index]),
                     visible=ifelse(descriptionsExports[index] %in% exportsToHighlight, TRUE, "legendonly"))
  fig <- add_ribbons(fig, 
                     x=summaryStats$Year,
                     ymin=summaryStats$UnitValue.Upper.97.5,
                     ymax=summaryStats$UnitValue.Lower.2.5,
                     name=descriptionsExportsShort[index], 
                     hovertemplate=labels,
                     line=list(color="transparent"),
                     fillcolor=setAlpha(coloursExports[index], 0.5),
                     visible=ifelse(descriptionsExports[index] %in% exportsToHighlight, TRUE, "legendonly"))
}

# Set initial plot title
fig <- layout(fig, title="Trends in unit value of principle commodities (<b>EXPORTS</b>)")

# Add initial X axis label
fig <- layout(fig, xaxis=list(title="Date"))

# Set Y axis to logged and label
fig <- layout(fig, yaxis=list(type="log", tickformat="f", title="Median unit value"))
fig
```

## Trends in the value of imports

```{r visualise principle commodity imports trends, echo=FALSE}
# Products to highlight
importsToHighlight <- c("Wood sawn or chipped lengthwise, sliced or peeled, whether or not planed, sanded or end-jointed",
                        "Cigars, cheroots, cigarillos and cigarettes, of tobacco or of tobacco substitutes")

# Start a plotly figure
fig <- plot_ly()

# Get the descriptions of principle commodities
descriptionsImports <- unique(principleImportsSummaryStats$Description)
descriptionsImportsShort <- substr(descriptionsImports, 1, 25)

# Define a colour for each principle commodity
palette <- colorRampPalette(brewer.pal(8, "Dark2"))
coloursImports <- palette(length(descriptionsImports)) 

# Add a trace for each principle commodity
for(index in seq_along(descriptionsImports)){
  
  # Get the data for current commodity
  summaryStats <- principleImportsSummaryStats[principleImportsSummaryStats$Description == descriptionsImports[index], ]
  
  # Add a trace for current commodity
  labels <- paste("<b>Year</b>:", summaryStats$Year,
                  "<br>Median: ", round(summaryStats$UnitValue.Median, digits=0),
                  "<br>Upper 97.5%: ", round(summaryStats$UnitValue.Upper.97.5, digits=0),
                  "<br>Lower 2.5%: ", round(summaryStats$UnitValue.Lower.2.5, digits=0))
  fig <- add_lines(fig, 
                     x=summaryStats$Year,
                     y=summaryStats$UnitValue.Median,
                     name=descriptionsImportsShort[index], 
                     hovertemplate=labels,
                     line=list(color=coloursImports[index]),
                     visible=ifelse(descriptionsImports[index] %in% importsToHighlight, TRUE, "legendonly"))
  fig <- add_ribbons(fig, 
                     x=summaryStats$Year,
                     ymin=summaryStats$UnitValue.Upper.97.5,
                     ymax=summaryStats$UnitValue.Lower.2.5,
                     name=descriptionsImportsShort[index], 
                     hovertemplate=labels,
                     line=list(color="transparent"),
                     fillcolor=setAlpha(coloursImports[index], 0.5),
                     visible=ifelse(descriptionsImports[index] %in% importsToHighlight, TRUE, "legendonly"))
}

# Set initial plot title
fig <- layout(fig, title="Trends in unit value of principle commodities (<b>IMPORTS</b>)")

# Add initial X axis label
fig <- layout(fig, xaxis=list(title="Date"))

# Set Y axis to logged and label
fig <- layout(fig, yaxis=list(type="log", tickformat="f", title="Median unit value"))
fig
```

# Concluding remarks

The histroci trades data provides a wealth of information that we can use to inform how we create the monthly trade statistics reports. We can see that the value of each commodity is highly skewed, making summary statistics like `median` and `percentiles` more appropriate. The value of each commodity is quite variable through time, it is potentially best to use last years data when defining the expected boundaries for each commodity.

We can quickly process the historic data, which has over a million rows using R and MySQL. We can use R to generate interactive visualisations that will help raise the value of these amazing data by make them more accessible.

# Closing a connection to the `MySQL` server

To wrap up, we can close our connection to the `MySQL` server using the following code:
```{r warning=FALSE}
dbDisconnect(conn=connection)
```